\documentclass[11pt,a4paper]{ctexart}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{environ}
\DeclareGraphicsExtensions{.pdf,.png,.jpg,.eps}
\graphicspath{{C:/Users/cengqQ/Pictures/}}
\linespread{1.6}
\CTEXsetup[indent={0pt}]{subparagraph}
\newcommand{\normal}[2]{\frac{1}{\sqrt{2\pi}#2}e^{-(x - #1)^2/(2#2^2)}}
\newcommand{\norm}[1]{\frac{1}{\sqrt{2\pi}#1}e^{-x^2/(2#1^2)}}
\newcommand{\norms}[0]{\frac{1}{\sqrt{2\pi}}e^{-x^2/2}}
\newcommand{\dconverge}[0]{\overset{\mathcal{D}}{\to}}
\newcommand{\pconverge}[0]{\overset{\mathcal{P}}{\to}}
\newcommand{\dd}[0]{\mathrm{d}}
\NewEnviron{formula}{ 
\begin{flalign*}\begin{split}
\BODY
\end{split}&\end{flalign*}}
\title{\vspace{-5ex}}
\author{基科32 曾柯又 2013012266}
\date{\vspace{-5ex}}
\begin{document}
	\section{损失函数作业}
\paragraph{7.65}
\subparagraph{(a)}
画出函数图像如下:
\begin{center}
	\includegraphics[scale=0.5]{L1}\\
	\includegraphics[scale=0.5]{L2}\\
	\includegraphics[scale=0.5]{L2}
\end{center}
	可见随着c的增加，函数图像逐渐由对称变得不对称。
\subparagraph{(b)}
\(E(L(\theta, \delta)|\mathbf{x}) = \mathrm{e}^{c\delta}E(\mathrm{e}^{-c\theta}|\mathbf{x}) - c(\delta - E(\theta|\mathbf{x})) - 1\)\\
求导可得\(c\mathrm{e}^{c\delta}E(\mathrm{e}^{-c\theta}|\mathbf{x}) -c = 0 \Rightarrow \delta = -\frac{1}{c}\log E(\mathrm{e}^{-c\theta}|\mathbf{x})\) \\
而\(c\mathrm{e}^{c\delta}E(\mathrm{e}^{-c\theta}|\mathbf{x}) -c\)随\(\delta\)的单调递增，故极值为极小值,满足要求，即：\(\delta^{\pi}(x) =-\frac{1}{c}\log E(\mathrm{e}^{-c\theta}|\mathbf{x})\)
\subparagraph{(c)}
这里的\(\pi(\theta)\)代表全实轴的均匀分布函数，可先设\(\pi(\theta) = \frac{1}{D}\)最后令\(D \to \infty\)(其实后面的计算与D无关)\\
\(\displaystyle 
f(\mathbf{x},\theta) = C\mathrm{e}^{-\sum_{i = 1}^{n}\frac{(x_i - \bar{x})^2}{2\sigma^2}}\mathrm{e}^{-\frac{n(\bar{x} - \theta)^2}{2\sigma^2}} = p(\mathbf{x})\mathrm{e}^{-\frac{n(\bar{x} - \theta)^2}{2\sigma^2}}\\
m(\mathbf{x}) = \int f(\mathbf{x}|\theta)\pi(\theta)\mathrm{d}\theta = p(\mathbf{x})\int \mathrm{e}^{-\frac{n(\bar{x} - \theta)^2}{2\sigma^2}}\mathrm{d}\theta\\
\pi(\theta|\mathbf{x}) = \frac{f(\mathbf{x},\theta)}{m(\mathbf{x})} = \frac{\mathrm{e}^{-\frac{n(\bar{x} - \theta)^2}{2\sigma^2}}}{\int\mathrm{e}^{-\frac{n(\bar{x} - \theta)^2}{2\sigma^2}}\mathrm{d}\theta}
\)\\
即\(\pi(\theta|\mathbf{x}) \sim \mathrm{n}(\bar{x},\frac{\sigma^2}{n})\)
可以得到\\
\(\displaystyle 
E(\mathrm{e}^{-c\theta}|\mathbf{x}) = M_{\theta|\mathbf{x}}(-c) = \mathrm{e}^{− c\bar{x}+\frac{\sigma^2c^2}{2n}} 
\\
\delta^B(\bar{X}) = \bar{X} - \frac{c\sigma^2}{2n}\)
\subparagraph{(d)}
\(\displaystyle
E(\theta|\mathbf{x}) = \bar{x} \\
E(L(\theta,\delta^B)|\mathbf{x}) =  \mathrm{e}^{c\delta}E(\mathrm{e}^{-c\theta}|\mathbf{x}) - c(\delta - E(\theta|\mathbf{x})) - 1 = \frac{c^2\sigma^2}{2n}\\
E(L(\theta,\bar{x})|\mathbf{x}) = \mathrm{e}^{c\bar{x}}E(\mathrm{e}^{-c\theta}|\mathbf{x}) - c(\bar{x} - E(\theta|\mathbf{x})) - 1 = \mathrm{e}^{\frac{\sigma^2c^2}{2n}} - 1
\)
\subparagraph{(e)}
\(
\displaystyle
L_2(\theta,a) =|\theta - a|^2 \\
E(L_2(\theta,\delta^B)|\mathbf{x}) = E\left[(\bar{x} - \theta)^2 + (\frac{c\sigma^2}{2n})^2 - 2(\bar{x} - \theta)\frac{c\sigma^2}{2n}\Big|x\right] = \frac{\sigma^2}{n} + \left(\frac{c\sigma^2}{2n}\right)^2\\
E(L_2(\theta,\bar{x})|\mathbf{x}) = E\left[(\bar{x} - \theta)^2\Big| x \right] = \frac{\sigma^2}{n}
\)
\paragraph{7.66}
\subparagraph{(a)}
\(\displaystyle \because \;n\bar{X} \sim \mathrm{B}(n,\theta) \mathrm{Var}(n\bar{X}) = E(n\bar{X})^2 - (En\bar{X})^2 \neq \theta^2\)
\subparagraph{(b)}
\(\displaystyle
\therefore E\bar{X}^2 = \frac{1}{n^2}E(n\bar{X})^2 = \frac{1}{n^2}\left[n\theta(1 - \theta) + n^2\theta^2\right] = \theta^2 + \frac{\theta(1 - \theta)}{n} \\
T_n= \bar{X}^2\\
T_n^{(i)} = \left(\frac{\sum_{k = 1}^{n}X_k - X_i}{n - 1}\right)^2 = \left(\frac{\sum_{k = 1}^{n}X_k}{n - 1}\right)^2 + \left(\frac{X_i}{n - 1}\right)^2 - \frac{2X_i(\sum_{k = 1}^{n}X_k)}{(n - 1)^2}\\
\sum_{i = 1}^{n}T_n^{(i)}  = \frac{(n - 2)(\sum_{k = 1}^{n}X_k)^2 + \sum_{i = 1}^{n}X_i^2}{(n - 1)^2} = \frac{(n - 2)n^2T_n + \sum_{i = 1}^{n}X_i^2}{(n - 1)^2}
\)
\begin{formula}
JK(T_n) &1= nT_n - \frac{n- 1}{n}\sum_{i = 1}^{n}T_n^{(i)}\\
& = nT_n - \frac{n(n - 2)T_n}{n - 1} - \frac{\sum_{i = 1}^{n}X_i^2}{n(n - 1)}\\
& = \frac{n}{n - 1}T_n - \frac{\sum_{i = 1}^{n}X_i^2}{n(n - 1)}
\end{formula}
而对0-1分布\(X_i = X_i^2\),故\(\displaystyle JK(T_n) =  \frac{n}{n - 1}T_n - \frac{\bar{X}}{(n - 1)}\)
\subparagraph{(c)}
\(EJK(T_n) = \frac{n}{n - 1}(\theta^2 + \frac{\theta(1 - \theta)}{n}) - \frac{\theta}{n - 1} = \theta\)
\subparagraph{(d)}
伯努利分布属于指数分布族，易知\(\bar{X}\)为\(\theta\)的完全充分统计量。

而\(\displaystyle JK(T_n) = \frac{n}{n - 1}\bar{X}^2 + \frac{\bar{X}}{n - 1}\)是\(\bar{X}\)的函数，因此\(JK(T_n)\)是\(\theta^2\)的最佳无偏估计
\paragraph{随机试验验证}
	利用R生成随机数，选取\(\theta =0.3,\, 0.5 ,\, 0.7\)，分别生成10个随机数(n 不能选的太大，否则\(\frac{\theta(1- \theta)}{n}\)趋近0而看不出区别)，计算估计量\(T_n , \,JK(T_n)\)，并重复10000次，再计算这10000次的平均值，可以验证上题的(a),(c)两小问。
	\begin{center}
		\begin{tabular}{|l|l|l|l|p{5cm}|}
			\hline
			mean & \( \theta = 0.3\) &\(\theta = 0.5 \)& \(\theta = 0.7\)\\ \hline
			\(\theta^2\) & 0.09 & 0.25 & 0.49 \\ \hline
			\(T_n\) & 0.11179 & 0.27567& 0.51027 \\ \hline
			\(JKT_n\) & 0.09072 & 0.25066 & 0.48928\\ \hline
		\end{tabular}
	\end{center}
	\(T_n\)的平均值明显比\(\theta^2\)要大，而\(JKT_n\)的平均值与\(\theta^2\)很接近。
\section{随机模拟：密度函数的估计}
\paragraph{1}
求出C的值为\(C = 1.0001397\), \(f\)的函数图像如下：
\begin{center}
	\includegraphics[scale=0.6]{fplot}
\end{center}
\paragraph{2}
将密度函数与直方图画在一张图上，可以看到抽样结果应当是正确的:
\begin{center}
	\includegraphics[scale = 0.5]{f100}\\
	\includegraphics[scale=0.5]{f1000}
\end{center}
\paragraph{}
计算出1000次模拟所得的RAISE值得均值和方差为如下表所示\\
\begin{center}
	\begin{tabular}{|c|c|c|p{7cm}|}
		\hline
		RAISE & mean & sd \\ \hline
		n = 100 &  0.01691 & 0.00301 \\ \hline
		n = 1000 & 0.00750& 0.00129\\ \hline
	\end{tabular}
\end{center}
而且可以发现RAISE的分布基本服从正态分布(证明显得有些困难)。直方图如下
\begin{center}
	\includegraphics[scale=0.5]{norm100}\\
	\includegraphics[scale=0.5]{norm1000}
\end{center}
可以看出取样数量n越大，密度函数的估计越好。这从直观上也容易理解。样本越多，
\end{document}
